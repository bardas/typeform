from fastapi import APIRouter, HTTPException
from transformers import pipeline
from pinecone import Pinecone
from app.utils.helper_functions import get_phi_role, prepare_prompt_phi
from app.services.retriever import retrieve_context
from app.schemas import QueryRequest, QueryResponse
from loguru import logger
from app.utils.helper_functions import load_data_config
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from transformers import AutoModelForCausalLM, AutoTokenizer

router = APIRouter()

index = None
embedder = None
model = None
tokenizer = None


def get_models():
    """
    Connect and load  index and embedder model.
    :return: Index and embedder model.
    """
    global index, embedder, model, tokenizer

    if all([index, embedder, model, tokenizer]):
        return index, embedder, model, tokenizer

    # get configs
    p_cfg = load_data_config(section="pinecone")
    em_cfg = load_data_config(section="embedding")
    llm_cfg = load_data_config(section="LLM")

    # connect to client
    pc = Pinecone(api_key=p_cfg.get("api_key"), environment=p_cfg.get("environment"))
    index_name = p_cfg.get("index_name")

    # Initialize embedder
    embed_model_name = em_cfg.get("model")
    embedder = HuggingFaceEmbedding(model_name=embed_model_name)

    # Connect
    index = pc.Index(index_name)
    # get model and tokenizer
    model_name = llm_cfg.get("model")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    return index, embedder, model, tokenizer


def generate_response_phi(
    prompt="", model=None, tokenizer=None, max_new_tokens=512, role=""
):
    """Function that generates a response for the microsoft/Phi-3.5-mini-instruct model
    args:
        prompt: str, prompt to be given to the model
        model: AutoModelForCausalLM, LLM model
        tokenizer: AutoTokenizer, LLM tokenizer
        max_new_tokens: the max tokens to be generated by the model
    returns:
        output: str, generated response
    """

    messages = [
        {"role": "system", "content": f"{role}"},
        {"role": "user", "content": f"{prompt}"},
    ]

    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=-1)

    generation_args = {
        "max_new_tokens": max_new_tokens,
        "return_full_text": False,
        "temperature": 0.9,
        "do_sample": True,
    }

    output = pipe(messages, **generation_args)
    output = output[0]["generated_text"].replace("<|end|>", "").strip()
    return output


@router.post("/query", response_model=QueryResponse)
def query_chatbot(req: QueryRequest):
    try:
        index, embedder, model, tokenizer = get_models()
        contexts = retrieve_context(
            question=req.question, index=index, embedder=embedder
        )
        prompt = prepare_prompt_phi(contexts, req.question)

        answer = generate_response_phi(
            prompt=prompt,
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            role=get_phi_role(),
        )
        return QueryResponse(answer=answer, sources=[])
    except Exception as e:
        logger.error(f"Query error: {e}")
        raise HTTPException(500, "Internal server error")
